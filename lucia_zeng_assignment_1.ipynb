{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zengluci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3248: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zengluci\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.235 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你去咖啡馆看书 is the best one with probability 5.232780885631001e-09\n",
      "你在教室买东西 is the worst one with probability 9.08468903755382e-12\n",
      "向东跳1000米 is the best one with probability 5.232780885631001e-09\n",
      "向南跳500米 is the worst one with probability 9.08468903755382e-12\n"
     ]
    }
   ],
   "source": [
    "navigator = \"\"\"\n",
    "recom = 连词 方向 动词 距离 单位\n",
    "方向 = 东 | 南 | 西 | 北  \n",
    "距离 = 100 | 500 | 2 | 1000\n",
    "单位 = 米 | 公里 \n",
    "动词 = 跑 | 走 | 跳 \n",
    "连词 = 朝 | 向 | 往\n",
    "\"\"\"\n",
    "\n",
    "daily = \"\"\"\n",
    "activity = 人物 连词 地点 活动\n",
    "人物 = 我 | 你 | 我们 | 她 | 他们\n",
    "连词 = 去 | 在 \n",
    "地点 = 超市 | 咖啡馆 | 购物中心 | 教室 | 医院\n",
    "活动 = 买东西 | 看病 | 看书 | 跳舞 | 聊天\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def create_grammar(grammar_str, stmt_split,  or_split):\n",
    "    rules={}\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "          # skip the empty line\n",
    "        stmt,expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = [s.strip() for s in expr.split(or_split)]\n",
    "    return rules\n",
    "\n",
    "\n",
    "def generate(grammar_rule, target):    \n",
    "    if target in grammar_rule: # names \n",
    "        candidates = grammar_rule[target]  # ['name names', 'name']\n",
    "        candidate = random.choice(candidates) #'name names', 'name'\n",
    "        return ''.join(generate(grammar_rule, target=c) for c in candidate.split() if c!='null')\n",
    "    else:\n",
    "        return target\n",
    "\n",
    "def generate_n(gram_name,stmt_split,or_split,target_name,size):\n",
    "    sents=[]\n",
    "    sen=\"\"\n",
    "    for i in range(size):\n",
    "        sen=generate(create_grammar(gram_name,stmt_split,or_split), target_name)\n",
    "        sents.append(sen)\n",
    "    return(sents)\n",
    "\n",
    "\n",
    "def token(string):\n",
    "     return re.findall('\\w+', string)\n",
    "    \n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: \n",
    "        return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)\n",
    "\n",
    "\n",
    "\n",
    "def get_probability(sentence):\n",
    "    words=cut(sentence)\n",
    "    sentence_pro=1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_=words[i+1]\n",
    "        probability= prob_2(word,next_)\n",
    "        sentence_pro*=probability\n",
    "    return sentence_pro\n",
    "\n",
    "\n",
    "def generate_best(sents):\n",
    "    sent_lib={}\n",
    "    for sen in sents:\n",
    "        sent_lib[sen]= get_probability(sen)\n",
    "   \n",
    "    result=sorted(sent_lib,key=lambda x: sent_lib[x],reverse=True)\n",
    "    print('{} is the best one with probability {}'.format(result[0],sent_lib[result[0]]))\n",
    "    print('{} is the worst one with probability {}'.format(result[-1],sent_lib[result[-1]]))  \n",
    " \n",
    "    return(result[0],sent_lib[result[0]],result[-1],sent_lib[result[-1]])\n",
    "\n",
    "\n",
    "def clean_file(train_file,clean_file):\n",
    "    content = pd.read_csv(train_file, encoding='UTF-8')\n",
    "    articles = content['comment'].tolist()\n",
    "    articles_clean = [''.join(token(str(a)))for a in articles]\n",
    "    with open(clean_file, 'w',encoding='utf-8') as f:\n",
    "        for a in articles_clean:\n",
    "            f.write(a + '\\n')\n",
    "            f.flush()\n",
    "    return()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    original_file='c:\\\\Users\\\\zengluci\\\\jupyters_and_slides\\\\2019-autumn\\\\deliverable\\\\movie_comments.csv'\n",
    "    target_file='c:\\\\Users\\\\zengluci\\\\jupyters_and_slides\\\\2019-autumn\\\\deliverable\\\\article_clean.csv'\n",
    "    clean_file(original_file,target_file)\n",
    "    FILE = open(target_file,'r',encoding='UTF-8').read()\n",
    "    max_length=1000\n",
    "    sub_file = FILE[:max_length]\n",
    "    TOKENS = cut(sub_file)\n",
    "    TOKEN_2_GRAM = [''.join(TOKENS[i:i+2]) for i in range(len(TOKENS[:-2]))]\n",
    "    words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "    generate_best(generate_n(gram_name=daily,stmt_split='=', or_split='|', target_name='activity',size=10))\n",
    "    generate_best(generate_n(gram_name=navigator,stmt_split='=', or_split='|', target_name='recom',size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
